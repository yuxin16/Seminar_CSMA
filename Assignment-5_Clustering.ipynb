{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFXsrSxJlzCW"
   },
   "source": [
    "#Introduction\n",
    "\n",
    "In this notebook we will use a clustering algorithm to analyze our data (i.e. YouTube comments of a single video).\n",
    "This will help us extract topics of discussion.\n",
    "We use the embeddings generated in Assignment 4 as input. \n",
    "(This notebook will not run without first running the assignment 4 Notebook, as it relies on the data in the folder 'output/')\n",
    "Each of our comments has been assigned a vector that encodes information about its meaning.\n",
    "The closer two vectors are, the more similar the meaning.\n",
    "\n",
    "Each vector is of 512 Dimensions.\n",
    "\n",
    "Before we can cluster our data we need to reduce the embeddings' dimensionality to overcome the curse of dimensionality.\n",
    "We use the UMAP ALgorithm for this.\n",
    "\n",
    "After that we use the KMedoids Algorithm to partition the embedding space and generate our clusters this way.\n",
    "\n",
    "We need to define the number of clusters we want to have. \n",
    "To find the optimal number of clusters, we use a simple optimization scheme.\n",
    "\n",
    "Once the clusters are created, we visualize them.\n",
    "To do this we reduce the dimensionality of the embeddings again to two dimensions.\n",
    "Then we render a scatterplot of our data.\n",
    "\n",
    "Furthermore we want to analyze and interpret our clusters.\n",
    "To do this, we:\n",
    "- print some statistics about each of the clusters\n",
    "- print cluster's medoid (the central sample)\n",
    "- print the cluster(s) we want to analyze further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if jupyter lab uses the correct python interpreter with '!which python'.\n",
    "It should be something like '/opt/anaconda3/envs/[environment name]/bin/python' (on Mac).\n",
    "If not, try this: https://github.com/jupyter/notebook/issues/3146#issuecomment-352718675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'which' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H0q7BTlbiCWW",
    "outputId": "32ddd123-4834-4913-f942-038e830d9276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\xinyu\\anaconda3\\envs\\Seminar_csma\n",
      "\n",
      "  added / updated specs:\n",
      "    - umap-learn\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2020.12.5  |       h5b45459_0         173 KB  conda-forge\n",
      "    certifi-2020.12.5          |   py36ha15d459_0         143 KB  conda-forge\n",
      "    llvmlite-0.35.0            |   py36haecd60e_0        15.1 MB  conda-forge\n",
      "    numba-0.52.0               |   py36he38d939_0         3.6 MB  conda-forge\n",
      "    openssl-1.1.1h             |       he774522_0         5.8 MB  conda-forge\n",
      "    umap-learn-0.4.6           |   py36h9f0ad1d_0         111 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        24.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  llvmlite           conda-forge/win-64::llvmlite-0.35.0-py36haecd60e_0\n",
      "  numba              conda-forge/win-64::numba-0.52.0-py36he38d939_0\n",
      "  tbb                conda-forge/win-64::tbb-2020.1-he980bc4_0\n",
      "  umap-learn         conda-forge/win-64::umap-learn-0.4.6-py36h9f0ad1d_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    anaconda::ca-certificates-2020.10.14-0 --> conda-forge::ca-certificates-2020.12.5-h5b45459_0\n",
      "  certifi                anaconda::certifi-2020.6.20-py36_0 --> conda-forge::certifi-2020.12.5-py36ha15d459_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  openssl                                          anaconda --> conda-forge\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "ca-certificates-2020 | 173 KB    |            |   0% \n",
      "ca-certificates-2020 | 173 KB    | 9          |   9% \n",
      "ca-certificates-2020 | 173 KB    | ########## | 100% \n",
      "ca-certificates-2020 | 173 KB    | ########## | 100% \n",
      "\n",
      "llvmlite-0.35.0      | 15.1 MB   |            |   0% \n",
      "llvmlite-0.35.0      | 15.1 MB   | 1          |   1% \n",
      "llvmlite-0.35.0      | 15.1 MB   | 4          |   4% \n",
      "llvmlite-0.35.0      | 15.1 MB   | 7          |   7% \n",
      "llvmlite-0.35.0      | 15.1 MB   | #          |  10% \n",
      "llvmlite-0.35.0      | 15.1 MB   | #2         |  12% \n",
      "llvmlite-0.35.0      | 15.1 MB   | #6         |  17% \n",
      "llvmlite-0.35.0      | 15.1 MB   | #9         |  19% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ##2        |  23% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ##5        |  26% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ##8        |  29% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ###1       |  32% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ###4       |  35% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ###7       |  38% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ####1      |  42% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ####4      |  45% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ####7      |  47% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ####9      |  50% \n",
      "llvmlite-0.35.0      | 15.1 MB   | #####3     |  53% \n",
      "llvmlite-0.35.0      | 15.1 MB   | #####6     |  56% \n",
      "llvmlite-0.35.0      | 15.1 MB   | #####9     |  59% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ######2    |  62% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ######5    |  65% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ######8    |  68% \n",
      "llvmlite-0.35.0      | 15.1 MB   | #######    |  71% \n",
      "llvmlite-0.35.0      | 15.1 MB   | #######3   |  74% \n",
      "llvmlite-0.35.0      | 15.1 MB   | #######6   |  77% \n",
      "llvmlite-0.35.0      | 15.1 MB   | #######9   |  80% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ########3  |  83% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ########6  |  86% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ########8  |  89% \n",
      "llvmlite-0.35.0      | 15.1 MB   | #########1 |  92% \n",
      "llvmlite-0.35.0      | 15.1 MB   | #########4 |  94% \n",
      "llvmlite-0.35.0      | 15.1 MB   | #########7 |  97% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ########## | 100% \n",
      "llvmlite-0.35.0      | 15.1 MB   | ########## | 100% \n",
      "\n",
      "umap-learn-0.4.6     | 111 KB    |            |   0% \n",
      "umap-learn-0.4.6     | 111 KB    | #4         |  14% \n",
      "umap-learn-0.4.6     | 111 KB    | ########## | 100% \n",
      "umap-learn-0.4.6     | 111 KB    | ########## | 100% \n",
      "\n",
      "openssl-1.1.1h       | 5.8 MB    |            |   0% \n",
      "openssl-1.1.1h       | 5.8 MB    |            |   0% \n",
      "openssl-1.1.1h       | 5.8 MB    | 7          |   8% \n",
      "openssl-1.1.1h       | 5.8 MB    | #1         |  11% \n",
      "openssl-1.1.1h       | 5.8 MB    | #6         |  16% \n",
      "openssl-1.1.1h       | 5.8 MB    | ##4        |  24% \n",
      "openssl-1.1.1h       | 5.8 MB    | ###        |  31% \n",
      "openssl-1.1.1h       | 5.8 MB    | ###9       |  40% \n",
      "openssl-1.1.1h       | 5.8 MB    | ####6      |  46% \n",
      "openssl-1.1.1h       | 5.8 MB    | #####2     |  52% \n",
      "openssl-1.1.1h       | 5.8 MB    | #####9     |  59% \n",
      "openssl-1.1.1h       | 5.8 MB    | ######5    |  66% \n",
      "openssl-1.1.1h       | 5.8 MB    | #######4   |  74% \n",
      "openssl-1.1.1h       | 5.8 MB    | ########   |  81% \n",
      "openssl-1.1.1h       | 5.8 MB    | ########7  |  88% \n",
      "openssl-1.1.1h       | 5.8 MB    | #########3 |  94% \n",
      "openssl-1.1.1h       | 5.8 MB    | ########## | 100% \n",
      "\n",
      "certifi-2020.12.5    | 143 KB    |            |   0% \n",
      "certifi-2020.12.5    | 143 KB    | ########## | 100% \n",
      "certifi-2020.12.5    | 143 KB    | ########## | 100% \n",
      "\n",
      "numba-0.52.0         | 3.6 MB    |            |   0% \n",
      "numba-0.52.0         | 3.6 MB    | 6          |   7% \n",
      "numba-0.52.0         | 3.6 MB    | #5         |  16% \n",
      "numba-0.52.0         | 3.6 MB    | ##4        |  25% \n",
      "numba-0.52.0         | 3.6 MB    | ###6       |  36% \n",
      "numba-0.52.0         | 3.6 MB    | ####8      |  48% \n",
      "numba-0.52.0         | 3.6 MB    | ######     |  61% \n",
      "numba-0.52.0         | 3.6 MB    | ######9    |  70% \n",
      "numba-0.52.0         | 3.6 MB    | ########1  |  81% \n",
      "numba-0.52.0         | 3.6 MB    | #########3 |  93% \n",
      "numba-0.52.0         | 3.6 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\xinyu\\anaconda3\\envs\\Seminar_csma\n",
      "\n",
      "  added / updated specs:\n",
      "    - scikit-learn-extra\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    scikit-learn-extra-0.1.0b2 |   py36hbd37c6d_0         124 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         124 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  scikit-learn-extra conda-forge/win-64::scikit-learn-extra-0.1.0b2-py36hbd37c6d_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "scikit-learn-extra-0 | 124 KB    |            |   0% \n",
      "scikit-learn-extra-0 | 124 KB    | #2         |  13% \n",
      "scikit-learn-extra-0 | 124 KB    | ########## | 100% \n",
      "scikit-learn-extra-0 | 124 KB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "install_packages = True\n",
    "if install_packages:    \n",
    "    !conda install -c conda-forge umap-learn -y\n",
    "    !conda install -c conda-forge scikit-learn-extra -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xD4HKjy9TC-g"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import seaborn as sns\n",
    "#from sklearn.cluster import AgglomerativeClustering, DBSCAN, KMeans, OPTICS\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Save and load manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load your data after clustering\n",
    "\n",
    "def save_results():\n",
    "    data.to_pickle(output_path+'data_clustered'+'.pkl')\n",
    "    \n",
    "def load_results():\n",
    "    data = pd.read_pickle(output_path+'data_clustered'+'.pkl')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas print options\n",
    "#This will improve readability of printed pandas dataframe.\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set global Parameters\n",
    "Set your parameters here:\n",
    "\n",
    "output_path: Files generated in this notebook will be saved here.\n",
    "\n",
    "model_type: Define which model was used to produce the embeddings. (Check the name of the .npy-file containing the embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TL7Rvq_TD086"
   },
   "outputs": [],
   "source": [
    "output_path = \"./output/\"\n",
    "model_type = 'Transformer' #@param ['DAN','Transformer','Transformer_Multilingual']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VO9QOLP_9DxJ"
   },
   "source": [
    "# Load Data\n",
    "Load the preprocessed data as a pandas dataframe.\n",
    "And load the embeddings as a numpy ndarray (a matrix in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './output/data_preprocessed.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-572d717f3ae4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'data_preprocessed'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlabels_default\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label_manual'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels_default\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\xinyu\\anaconda3\\envs\\Seminar_csma\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"infer\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# 1) try standard library Pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\xinyu\\anaconda3\\envs\\Seminar_csma\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './output/data_preprocessed.pkl'"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle(output_path+'data_preprocessed'+'.pkl')\n",
    "labels_default = np.zeros(len(data.index))-1\n",
    "data['label_manual'] = labels_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load(output_path+'/embeddings'+model_type+'.npy', mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IUWISh1M5nz"
   },
   "source": [
    "# Dimensionality reduction with UMAP\n",
    "\n",
    "We reduce the number of dimensions of our embeddings to make possibly present clusters more pronounced. \n",
    "The number of dimensions (num_dimensions) depends on the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of dimensions to reduce to\n",
    "num_dimensions =100\n",
    "\n",
    "reducer_clustering = umap.UMAP(n_neighbors=50, \n",
    "                     n_components=num_dimensions, \n",
    "                     metric='cosine', \n",
    "                     #n_epochs=200, \n",
    "                     learning_rate=.5,\n",
    "                     init='spectral', \n",
    "                     min_dist=0,\n",
    "                     #spread=5.0, \n",
    "                     #set_op_mix_ratio=1.0, \n",
    "                     #local_connectivity=1.0, \n",
    "                     #negative_sample_rate=5, \n",
    "                     #transform_queue_size=4.0, \n",
    "                     force_approximation_algorithm=True, \n",
    "                     unique=True)\n",
    "embeddings_umap = reducer_clustering.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimize number of clusters\n",
    "optimize_number_of_clusters = True#@param {type:'boolean'}\n",
    "\n",
    "min_clusters=2 \n",
    "max_clusters=1000 \n",
    "step=100 \n",
    "\n",
    "if optimize_number_of_clusters:\n",
    "    rows_list = []\n",
    "    inertias = []\n",
    "    n_clusters = []\n",
    "    silouette_scores = []\n",
    "    init_param = 'k-medoids++' #@param ['random', 'heuristic', 'k-medoids++']\n",
    "    random_state_param=1234 #@param {type:'number'}\n",
    "    for i in range(min_clusters,max_clusters, step):\n",
    "        temp_clustering = KMedoids(n_clusters=i, metric='euclidean', init=init_param, max_iter=200, random_state=random_state_param).fit(embeddings_umap)\n",
    "        silhouette_avg = silhouette_score(embeddings_umap, temp_clustering.labels_)\n",
    "        print(\"n_clusters:\",i, \"silhouette_avg:\",silhouette_avg)\n",
    "        silhouette_dict = {'number of clusters': i, 'silhouette average': silhouette_avg}\n",
    "        rows_list.append(silhouette_dict)\n",
    "    results = pd.DataFrame(rows_list)\n",
    "    sns.lineplot(x = 'number of clusters', y = 'silhouette average',data = results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with KMedoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters =  100\n",
    "init_param = 'k-medoids++' #@param ['random', 'heuristic', 'k-medoids++']\n",
    "clustering_model = KMedoids(n_clusters=number_of_clusters,\n",
    "                            metric='cosine', \n",
    "                            init=init_param, \n",
    "                            max_iter=150, \n",
    "                            random_state=None).fit(embeddings_umap)\n",
    "clustering_model\n",
    "labels = clustering_model.labels_\n",
    "data[\"label_kmedoids\"] = labels\n",
    "print(\"cluster\",\"members\", data[\"label_kmedoids\"].value_counts().sort_values())\n",
    "\n",
    "clustering_model.inertia_\n",
    "\n",
    "medoids_indices = clustering_model.medoid_indices_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate distances\n",
    "distances = np.diag(pairwise_distances(X = clustering_model.cluster_centers_[labels], Y = embeddings_umap[:], metric='cosine'))\n",
    "data[\"distance_kmedoids\"] = distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dimensions =2\n",
    "\n",
    "reducer_visualization = umap.UMAP(n_neighbors=50, \n",
    "                                  n_components=num_dimensions, \n",
    "                                  metric='cosine', \n",
    "                                  output_metric='euclidean', \n",
    "                                  #n_epochs=200, \n",
    "                                  learning_rate=.5,\n",
    "                                  init='spectral', \n",
    "                                  min_dist=.1,\n",
    "                                  spread=5.0, \n",
    "                                  set_op_mix_ratio=1.0, \n",
    "                                  local_connectivity=1.0, \n",
    "                                  negative_sample_rate=5, \n",
    "                                  transform_queue_size=4.0, \n",
    "                                  force_approximation_algorithm=True, \n",
    "                                  unique=True)\n",
    "embeddings_umap_2d = reducer_visualization.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Set the color palette used for visualizing different clusters\n",
    "palette_param = \"Accent\" #@param ['Accent','cubehelix', \"tab10\", 'Paired', \"Spectral\"]\n",
    "#@markdown Set opacity of data points (1 = opaque, 0 = invisible)\n",
    "alpha_param = 0.16 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "\n",
    "\n",
    "sns.relplot(x = embeddings_umap_2d[:, 0], y = embeddings_umap_2d[:, 1],  hue = data['label_kmedoids'], palette = palette_param,alpha = alpha_param,height = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highlight one cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose a cluster to higlight:\n",
    "cluster_num = 6\n",
    "\n",
    "data['highlight'] = np.zeros(len(data.index))\n",
    "data.loc[data['label_kmedoids'] == cluster_num, 'highlight'] = 1\n",
    "\n",
    "sns.relplot(x = embeddings_umap_2d[:, 0], y = embeddings_umap_2d[:, 1],  hue = data['highlight'], palette = \"Accent\",alpha = 0.8,height = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Medoids and cluster statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the medoids \n",
    "data.iloc[medoids_indices]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print statistics for each cluster\n",
    "data['label_kmedoids'].value_counts().sort_values()\n",
    "for k,g in data.groupby(by = 'label_kmedoids'):\n",
    "      print(g.iloc[0]['label_kmedoids'],\"number of samples: \",len(g.index),\"mean distance from center: \", 100*np.mean(g['distance_kmedoids']), \"Proportion of replies:\", 100*np.sum(g['isReply'])/len(g.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Cluster\n",
    "Print the comments within a cluster. Comments are sorted by their distance from the cluster medoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a cluster to print\n",
    "cluster_number =   20\n",
    "# Choose the number of samples to print\n",
    "number_of_samples_to_print = 10000\n",
    "\n",
    "data['label_kmedoids'] = data['label_kmedoids'].astype('category')\n",
    "cluster = data[data['label_kmedoids']==cluster_number]\n",
    "if cluster[\"text\"].count()<=number_of_samples_to_print:\n",
    "  number_of_samples_to_print = cluster[\"text\"].count()\n",
    "\n",
    "cluster = cluster.sort_values(by='distance_kmedoids')\n",
    "\n",
    "print(\"Number of samples in the cluster:\", cluster[\"text\"].count())\n",
    "print(\"Average Distance from cluster center:\", np.mean(cluster['distance_kmedoids']))\n",
    "cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign Cluster labels manually\n",
    "cluster_number: which cluster would you like to assign labels to?\n",
    "min_distance: the minimum distance from the cluster medoid be for a data point to still get the specified label\n",
    "max_distance: the maximum distance from the cluster medoid be for a data point to still get the specified label\n",
    "label_manual: your label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which cluster would you like to assign labels to?\n",
    "cluster_number = 18\n",
    "#your label\n",
    "label_manual = 'music'\n",
    "#the minimum distance from the cluster medoid be for a data point to still get the specified label\n",
    "min_distance = 0\n",
    "#the maximum distance from the cluster medoid be for a data point to still get the specified label\n",
    "max_distance = 1000\n",
    "\n",
    "# 2. Filter data by cluster label and specified label to filtered data\n",
    "data.loc[(data['label_kmedoids']==cluster_number) & (data['distance_kmedoids'] <= max_distance) & (data['distance_kmedoids'] >= min_distance),  'label_manual'] = label_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['label_kmedoids']==cluster_number].sort_values(by='distance_kmedoids')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "USE_Experiments_new.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
